{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409897f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9f6611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "data = load_breast_cancer()\n",
    "x=data.data\n",
    "y=data.target\n",
    "print(\"shape of data:\",x.shape)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "x_new=scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad58d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size = 0.10, random_state=2021)\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78971d4c",
   "metadata": {},
   "source": [
    "**1) Logistic Regression**: \n",
    "its a classification method. Classification is a supervised learning method from machine learning, that tries to predict to which class a certain entity belongs to, based on its features.\n",
    "\n",
    "Logistic regression determines the best predicted weights 𝑏₀, 𝑏₁, …, 𝑏ᵣ such that the function 𝑝(𝐱) is as close as possible to all actual responses 𝑦ᵢ, 𝑖 = 1, …, 𝑛, where 𝑛 is the number of observations. The process of calculating the best weights using available observations is called *model training* or *fitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03406efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights\n",
    "def weights_bias(shape):\n",
    "    weights = np.full((shape,1),0.1)\n",
    "    bias = 1\n",
    "    return weights,bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6634f",
   "metadata": {},
   "source": [
    "The **sigmoid function** has values very close to either 0 or 1 across most of its domain. Our goal is to find the logistic regression function p(x), such that the predicted responses 𝑝(𝐱ᵢ) are as close as possible to the actual response 𝑦ᵢ (either 0 or 1) for each observation 𝑖 = 1, …, 𝑛. Therefore, it is convenient to use the sigmoid function, so each 𝑝(𝐱ᵢ) is close to either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d85ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(z):  \n",
    "    y_predict = 1/(1+ np.exp(-z))\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a122c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w...weights\n",
    "# b...bias\n",
    "\n",
    "def forward_backward(w,b,x_train,y_train):\n",
    "    '''calculates the gradient of weight and bias\n",
    "    returns a dictionary with derivatives'''\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_predict = sigmoid(z)\n",
    "    derivative_weight = (np.dot(x_train,((y_predict-y_train).T)))/x_train.shape[1]\n",
    "    derivative_bias = np.sum(y_predict-y_train)/x_train.shape[1]                 \n",
    "    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b028be",
   "metadata": {},
   "source": [
    "We use **GD** to find the **optimal weights** and update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca97515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w, b, x_train, y_train, learning_rate,iterations):\n",
    "    for i in range(iterations):\n",
    "        gradients = forward_backward(w,b,x_train,y_train)\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    return parameters, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e305178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,x_test):\n",
    "    '''Calculates the predicted weights with help of the sigmoid function'''\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            y_prediction[0,i] = 0\n",
    "        else:\n",
    "            y_prediction[0,i] = 1\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "801be45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate,  iterations):\n",
    "    shape =  x_train.shape[0]\n",
    "    w,b = weights_bias(shape)\n",
    "    parameters, gradients = update_parameters(w, b, x_train, y_train, learning_rate,iterations)\n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    loss = np.mean(np.abs(y_prediction_test - y_test)) * 100\n",
    "    accuracy = 100 - np.mean(np.abs(y_prediction_test - y_test)) * 100\n",
    "    print(\"test accuracy: {}% \".format(accuracy))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180d6ab",
   "metadata": {},
   "source": [
    "**Loss function**: measures how much the prediction differs from the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47c77736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 71.9298245614035% \n",
      "test accuracy: 71.9298245614035% \n",
      "test accuracy: 73.6842105263158% \n",
      "test accuracy: 77.19298245614036% \n",
      "test accuracy: 78.94736842105263% \n",
      "test accuracy: 78.94736842105263% \n",
      "test accuracy: 80.70175438596492% \n",
      "test accuracy: 82.45614035087719% \n",
      "test accuracy: 87.71929824561404% \n",
      "test accuracy: 89.47368421052632% \n",
      "test accuracy: 89.47368421052632% \n",
      "test accuracy: 89.47368421052632% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "[31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 29.82456140350877, 29.82456140350877, 29.82456140350877, 29.82456140350877, 29.82456140350877, 29.82456140350877, 29.82456140350877, 28.07017543859649, 28.07017543859649, 26.31578947368421, 22.807017543859647, 21.052631578947366, 21.052631578947366, 19.298245614035086, 17.543859649122805, 12.280701754385964, 10.526315789473683, 10.526315789473683, 10.526315789473683, 8.771929824561402, 8.771929824561402, 8.771929824561402, 8.771929824561402, 8.771929824561402, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842]\n"
     ]
    }
   ],
   "source": [
    "loss_values=[]\n",
    "for i in range(1,150):\n",
    "    val = [logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.1, iterations = i)]\n",
    "    loss_values+= val\n",
    "\n",
    "    i+=1\n",
    "print(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe2ecb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_values)\n",
    "plt.title('loss vs. no. of epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1f395",
   "metadata": {},
   "source": [
    "**2) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea231e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
