{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409897f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9f6611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: (569, 30)\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "data = load_breast_cancer()\n",
    "x=data.data\n",
    "y=data.target\n",
    "print(\"shape of data:\",x.shape)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler=MinMaxScaler()\n",
    "x_new=scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad58d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_new, y, test_size = 0.10, random_state=2021)\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78971d4c",
   "metadata": {},
   "source": [
    "**1) Logistic Regression**: \n",
    "its a classification method. Classification is a supervised learning method from machine learning, that tries to predict to which class a certain entity belongs to, based on its features.\n",
    "\n",
    "Logistic regression determines the best predicted weights ğ‘â‚€, ğ‘â‚, â€¦, ğ‘áµ£ such that the function ğ‘(ğ±) is as close as possible to all actual responses ğ‘¦áµ¢, ğ‘– = 1, â€¦, ğ‘›, where ğ‘› is the number of observations. The process of calculating the best weights using available observations is called *model training* or *fitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03406efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights\n",
    "def weights_bias(shape):\n",
    "    weights = np.full((shape,1),0.1)\n",
    "    bias = 1\n",
    "    return weights,bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a6634f",
   "metadata": {},
   "source": [
    "The **sigmoid function** has values very close to either 0 or 1 across most of its domain. Our goal is to find the logistic regression function p(x), such that the predicted responses ğ‘(ğ±áµ¢) are as close as possible to the actual response ğ‘¦áµ¢ (either 0 or 1) for each observation ğ‘– = 1, â€¦, ğ‘›. Therefore, it is convenient to use the sigmoid function, so each ğ‘(ğ±áµ¢) is close to either 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24d85ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(z):  \n",
    "    y_predict = 1/(1+ np.exp(-z))\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a122c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w...weights\n",
    "# b...bias\n",
    "\n",
    "def forward_backward(w,b,x_train,y_train):\n",
    "    '''calculates the gradient of weight and bias\n",
    "    returns a dictionary with derivatives'''\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_predict = sigmoid(z)\n",
    "    derivative_weight = (np.dot(x_train,((y_predict-y_train).T)))/x_train.shape[1]\n",
    "    derivative_bias = np.sum(y_predict-y_train)/x_train.shape[1]                 \n",
    "    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b028be",
   "metadata": {},
   "source": [
    "We use **GD** to find the **optimal weights** and update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca97515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(w, b, x_train, y_train, learning_rate,iterations):\n",
    "    for i in range(iterations):\n",
    "        gradients = forward_backward(w,b,x_train,y_train)\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    return parameters, gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e305178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,x_test):\n",
    "    '''Calculates the predicted weights with help of the sigmoid function'''\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            y_prediction[0,i] = 0\n",
    "        else:\n",
    "            y_prediction[0,i] = 1\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "801be45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate,  iterations):\n",
    "    shape =  x_train.shape[0]\n",
    "    w,b = weights_bias(shape)\n",
    "    parameters, gradients = update_parameters(w, b, x_train, y_train, learning_rate,iterations)\n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    loss = np.mean(np.abs(y_prediction_test - y_test)) * 100\n",
    "    accuracy = 100 - np.mean(np.abs(y_prediction_test - y_test)) * 100\n",
    "    print(\"test accuracy: {}% \".format(accuracy))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4180d6ab",
   "metadata": {},
   "source": [
    "**Loss function**: measures how much the prediction differs from the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47c77736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 68.42105263157895% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 70.17543859649123% \n",
      "test accuracy: 71.9298245614035% \n",
      "test accuracy: 71.9298245614035% \n",
      "test accuracy: 73.6842105263158% \n",
      "test accuracy: 77.19298245614036% \n",
      "test accuracy: 78.94736842105263% \n",
      "test accuracy: 78.94736842105263% \n",
      "test accuracy: 80.70175438596492% \n",
      "test accuracy: 82.45614035087719% \n",
      "test accuracy: 87.71929824561404% \n",
      "test accuracy: 89.47368421052632% \n",
      "test accuracy: 89.47368421052632% \n",
      "test accuracy: 89.47368421052632% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 91.2280701754386% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 92.98245614035088% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "test accuracy: 94.73684210526316% \n",
      "[31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 31.57894736842105, 29.82456140350877, 29.82456140350877, 29.82456140350877, 29.82456140350877, 29.82456140350877, 29.82456140350877, 29.82456140350877, 28.07017543859649, 28.07017543859649, 26.31578947368421, 22.807017543859647, 21.052631578947366, 21.052631578947366, 19.298245614035086, 17.543859649122805, 12.280701754385964, 10.526315789473683, 10.526315789473683, 10.526315789473683, 8.771929824561402, 8.771929824561402, 8.771929824561402, 8.771929824561402, 8.771929824561402, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 7.017543859649122, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842, 5.263157894736842]\n"
     ]
    }
   ],
   "source": [
    "loss_values=[]\n",
    "for i in range(1,150):\n",
    "    val = [logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.1, iterations = i)]\n",
    "    loss_values+= val\n",
    "\n",
    "    i+=1\n",
    "print(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe2ecb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeoUlEQVR4nO3de5RcZZ3u8e/T16SrQkinqyEEQkiFw4AoQQOiqAsBHWQYuRxUEDgs5cjMGh1hxjmK4plB5nJwqajH42WiMkZFUEGEgw6DxtswcjHJ4WpkICFAQkg6QO5JJ935nT/2bmhCd6eSTtWuqv181qrVVW/ty6930k/vfvdb71ZEYGZm+dGSdQFmZlZbDn4zs5xx8JuZ5YyD38wsZxz8ZmY54+A3M8sZB7+Nm6Tlkk7Nuo5GI+lESY9J2iTprDqoZ6akkNSWdS1WXQ5+s+xcDfyfiChGxI+zLsbyw8Fvlp1DgUeyLsLyx8Fv+5SkTklfkPRM+viCpM70vR5Jt0taJ+l5Sf8uqSV972OSVkraKOlRSaeMsO0TJD0rqXVY29mSHkyfHy9poaQNklZLurbCmq+S9ANJ3073/4ikucPeP1LSr9K6H5H0zj04Hh+Q9Hj6/d4m6aC0fSkwC/i/aVdP5wjrHiTpZkl9kp6Q9OFdar5J0vfTmhdLOqaSmiVNlPQ5SU9KWi/pLkkTh+36AklPSVor6cph6+3V8bU6FBF++DGuB7AcODV9fjVwD9ALlIDfAn+fvve/gK8B7enjzYCAI4CngYPS5WYC5VH2tRR427DXPwSuSJ/fDVyUPi8CJ1RY/1XANuB0oDWt8570vXbgceATQAdwMrAROKKC7Z4MrAVeC3QCXwJ+M9JxG2HdFmAR8LfpfmcBy4A/HlbzDuDctMa/AZ4YdmxHrRn4MvArYHr6/b4xrW8mEMDXgYnAMUA/cOR4jq8f9ffwGb/taxcAV0fEmojoAz4FXJS+twOYBhwaETsi4t8jSZFBkuA5SlJ7RCyPiKWjbP8G4HwASZNIwvqGYdufLaknIjZFxD17UPddEfHTiBgEvkMSegAnkITcNRGxPSJ+Adw+VMNuXABcFxGLI6If+DjwBkkzK1j3OKAUEVen+11GEsjnDVtmUUTcFBE7gGuBCWm9o9ac/oX1fuCyiFgZEYMR8du0viGfioitEfEA8MCwYzGe42t1xMFv+9pBwJPDXj+ZtgF8huRM9E5JyyRdARARjwOXk5zFrpF041CXyAi+B5yTdo2cAyyOiKH9XQL8F+APkn4n6Yw9qPvZYc+3ABPS0S0HAU9HxM5dvqfpFWzzZcciIjYBz1W47qHAQWlXzTpJ60jO4A8YtszTw7a9E1iR7nOsmntIfkGM9osVXnksiunz8RxfqyMOftvXniEJrSEz0jYiYmNEfCQiZgF/Cvz1UF9+RHwvIt6UrhvAp0faeET8niTE3gG8l+QXwdB7j0XE+STdTJ8GbpJU2AffzyFD1yKGfU8rK1z3xWOR1jK1wnWfBp6IiP2HPSZFxOnDljlk2LZbgIPTfY5V81qSbq1yBTW8TJWOr2XAwW/72g3AJyWVJPWQ9FF/F0DSGZJmSxKwgaSLZ1DSEZJOTs/itwFb0/dG8z3gw8BbSPr4Sbd/oaRSeqa7Lm0eazuVuBfYDHxUUrukk0h+ad1YwbrfA94naU76vf0TcG9ELK9g3fuADelF74mSWiUdLem4Ycu8TtI56V8ml5P0x98zVs3psbkOuDa9eNwq6Q0jXVzeVZWOr2XAwW/72j8AC4EHgYeAxWkbwOHAz4FNJBcKvxIRvyLp37+G5Gz0WZIzyk+MsY8bgJOAX0TE2mHtpwGPSNoEfBE4LyK2AaQjZ968p99MRGwH3knyF8Za4CvAf4uIP6Tb/VdJI9YaEQuA/wncDKwiOcs+b6RlR1h3kCSs55BctF0LfAOYPGyxW4H3AC+QXEc5J712MmbNJBeCHwJ+BzxPcvZeSRaMenytsSi5tmZmjUTSVcDsiLgw61qs8fiM38wsZxz8ZmY5464eM7Oc8Rm/mVnONMT0qz09PTFz5sysyzAzayiLFi1aGxGlXdsbIvhnzpzJwoULsy7DzKyhSHpypHZ39ZiZ5YyD38wsZxz8ZmY54+A3M8sZB7+ZWc44+M3McsbBb2aWMw0xjn9vLViymgeeXpd1GaOaVSpy1rGV3IzJzGzfaerg//V/9vGde0b8/ELmhqZIettRB1DobOp/BjOrM02dOFefeTRXn3l01mWM6I6HV/Hn313ME2s3c/T0ybtfwcxsH3Eff0bKpeT+1Uv7NmVciZnljYM/IzOmdtHaIpaucfCbWW05+DPS2dbKjO4ulvZtzroUM8sZB3+GZvUU3NVjZjXn4M9QubfIsrWbGdzpu6CZWe04+DNULhXYPrCTlS9szboUM8sRB3+GPLLHzLJQteCXNEHSfZIekPSIpE+l7d2SfibpsfTrlGrVUO8c/GaWhWqe8fcDJ0fEMcAc4DRJJwBXAAsi4nBgQfo6l6YUOugudDj4zaymqhb8kRhKtPb0EcCZwPy0fT5wVrVqaATlUoGlazyk08xqp6pTNkhqBRYBs4EvR8S9kg6IiFUAEbFKUu8o614KXAowY8aMapaZqXKpyB2PPMtPH1pV8TrtrS28+fAeJrS3VrEyM2tWVQ3+iBgE5kjaH7hFUsUT50TEPGAewNy5c5t2vOPR0ydz4++e5i+uX7xH611zzqs57/jm/YVoZtVTk0naImKdpF8BpwGrJU1Lz/anAWtqUUO9eu/xMzhh1tSKx/IHwTlf+S2Prt5Y5crMrFlVLfgllYAdaehPBE4FPg3cBlwMXJN+vbVaNTSClhYxu7e4R+uUS0VP9WBme62aZ/zTgPlpP38L8IOIuF3S3cAPJF0CPAW8q4o1NKVZpQILl7+QdRlm1qCqFvwR8SBw7AjtzwGnVGu/eVAuFbn1/mfYun2QiR2+wGtme8af3G1AQx/8WrbW4//NbM85+BtQubcA4H5+M9srDv4GNHNqAQnfxMXM9oqDvwFNaG/lkCldnurBzPaKg79BlUsFd/WY2V5x8DeocqnIsr5N7PRNXMxsDzn4G1S5t0j/wE5WrvNNXMxszzj4G9TQkM67lz3H8rWbfeZvZhWryVw9tu/N7i0iwUdvehCAK08/kg+8ZVbGVZlZI3DwN6juQgc3fuAEnt2wjX/8yRIeWrk+65LMrEE4+BvY62dNBeBHi1d6aKeZVcx9/E0gGeHjfn4zq4yDvwmUewts3THIqg3bsi7FzBqAg78JDI3w8RQOZlYJB38TeDH43c9vZhVw8DeBnmIH+01oY5mncDCzCjj4m4Akyr1Fn/GbWUUc/E0iuQ+vg9/Mds/B3yTKpSKrN/SzcduOrEsxszrn4G8S5VJyVy7385vZ7jj4m0S51yN7zKwyDv4mMaO7i7YWOfjNbLcc/E2ivbWFnmInfRv7sy7FzOqcg7+JTCl08PxmX9w1s7E5+JtId6GdF7Zsz7oMM6tzDv4mMqWrgxc2O/jNbGwO/ibSXejgeZ/xm9luOPibyJSuDtZv3cHA4M6sSzGzOubgbyLdhQ4iYP1WX+A1s9FVLfglHSLpl5KWSHpE0mVp+1WSVkq6P32cXq0a8mZKoQPAF3jNbEzVvOfuAPCRiFgsaRKwSNLP0vc+HxGfreK+c6m7Kwl+D+k0s7FULfgjYhWwKn2+UdISYHq19mcwpdAOwPMe2WNmY6hJH7+kmcCxwL1p04ckPSjpOklTRlnnUkkLJS3s6+urRZkNr9tdPWZWgaoHv6QicDNweURsAL4KlIE5JH8RfG6k9SJiXkTMjYi5pVKp2mU2hSkvdvU4+M1sdFUNfkntJKF/fUT8CCAiVkfEYETsBL4OHF/NGvJkQnsrXR2t/hCXmY2pmqN6BHwTWBIR1w5rnzZssbOBh6tVQx5N6fKHuMxsbNUc1XMicBHwkKT707ZPAOdLmgMEsBz4syrWkDvdBU/bYGZjq+aonrsAjfDWT6u1T4P9u9p5fouHc5rZ6PzJ3SbjM34z2x0Hf5PxDJ1mtjsO/ibTXehgY/8A2wc8UZuZjczB32SG5utZ55E9ZjYKB3+TeXG+Hge/mY3Cwd9kPF+Pme2Og7/JvDhfj2foNLNROPibjLt6zGx3HPxNZv+uoTN+B7+ZjczB32Q62lqY1NnmPn4zG5WDvwn17tfJM+u2Zl2GmdUpB38TKpeKLFu7OesyzKxOOfibULm3yJPPbWZg0J/eNbNXcvA3oXKpyI7B4OkX3N1jZq/k4G9Cs0oFAJau2ZRxJWZWjxz8TajcUwRgaZ+D38xeycHfhCZ3tdNT7HTwm9mIHPxNqlwqsLTPI3vM7JUc/E2q3Fvk8TWbiIisSzGzOuPgb1LlUpH1W3f4E7xm9goO/iZVHhrZ4+4eM9uFg79JlUse2WNmI2vLugCrjun7T6SzrYWfPLiKLdsHeWN5KkdO2y/rssysDjj4m1RLizhuZjd3Pb6Wux5fy3Ezp/DDP39j1mWZWR1w8Dex+e8/nk39A/z97b9nwZLVWZdjZnXCffxNrLVFTJ7Yzh8dOIkXtniEj5klHPw5UO71hV4ze4mDPwdmpyN8ljn4zYwqBr+kQyT9UtISSY9Iuixt75b0M0mPpV+nVKsGSxyUjvDxmH4zgz0MfkmnSPpTSe0VLD4AfCQijgROAD4o6SjgCmBBRBwOLEhfWxW1tojDegqeptnMgD0IfkmfA04lCfFbd7d8RKyKiMXp843AEmA6cCYwP11sPnDWnpVse6PcW3Qfv5kBYwS/pM9KmjysaQbwCeCT6fOKSZoJHAvcCxwQEasg+eUA9I6yzqWSFkpa2NfXtye7sxGUS0Ween4L/QODWZdiZhkb64z/FuD7kv5SUivwbeAe4H5gXqU7kFQEbgYuj4gNla4XEfMiYm5EzC2VSpWuZqMolwrsDHjyuS1Zl2JmGRs1+CPiPyLiNGAdcEfa9vqIOCYi/nclG0+vBdwMXB8RP0qbV0ualr4/DVgzjvqtQi/O3eN+frPcG6urp03SnwCrgbOBYyXdJuk1lWxYkoBvAksi4tphb90GXJw+v5gKrhfY+B3WMzRbp4PfLO/GmrLhxyTdOl3ABRFxsaSDgKslRUR8YDfbPhG4CHhI0v1p2yeAa4AfSLoEeAp4196Xb5UqdLZx0OQJHtJpZmMG/6ERcYakDpK+fSLiGeC/S5qzuw1HxF2ARnn7lD0t1MbPI3vMDMa+uDsvPVO/FxjeVUNE3F/FmqxKyqUiS307RrPcG/WMPyK+BHyphrVYlZVLBTZvH2T1hn4OnDwh63LMLCOeqydHfFcuMwMHf654lk4zAwd/rvRO6qTY2eax/GY5t9vgl3SZpP2U+KakxZLeXovibN+SRLlU8JBOs5yr5Iz//elUC28HSsD7SMbiWwMqlzyk0yzvKgn+obH4pwP/EhEPMPr4fKtz5d4iq9ZvY1P/QNalmFlGKgn+RZLuJAn+f5M0CdhZ3bKsWsqlZOqGJ9zdY5ZblQT/JSQ3SzkuIrYA7STdPdaAhoZ0Llvr7h6zvKok+N8APBoR6yRdSDIf//rqlmXVMmNqF60t8sgesxyrJPi/CmyRdAzwUeBJkrn5rQF1trUyo7vLI3vMcqyS4B+IZHKXM4EvRsQXgUnVLcuqaVZPwSN7zHKskuDfKOnjJFMs/yS9G1clN1u3OjWl0MGGrTuyLsPMMlJJ8L8H6CcZz/8syQ3TP1PVqqyqip1tbPRwTrPc2m3wp2F/PTBZ0hnAtohwH38DK3a2sbl/wNMzm+VUJVM2vBu4j+ROWe8G7pV0brULs+opdLaxM2DbDn8cwyyPxroD15ArScbwrwGQVAJ+DtxUzcKseoqdrQBs7N/BxI7WjKsxs1qrpI+/ZSj0U89VuJ7VqeKE5Pf95v7BjCsxsyxUcsZ/h6R/A25IX78H+Gn1SrJqK3QMBb8v8Jrl0W6DPyL+h6T/CpxIMjnbvIi4peqVWdUUO5N/9o3bHPxmeVTJGT8RcTNwc5VrsRp5qavHwW+WR6MGv6SNwEjj/QREROxXtaqsqgrpGf/m7Q5+szwaNfgjwtMyNCl39Zjlm0fn5NBQ8LurxyyfHPw51NXRiuTgN8srB38OSaLQ4fl6zPLKwZ9TQ/P1mFn+OPhzqtDZ6k/umuVU1YJf0nWS1kh6eFjbVZJWSro/fZxerf3b2Dw1s1l+VfOM/1vAaSO0fz4i5qQPT/2QkeIEd/WY5VXVgj8ifgM8X63t2/gUOhz8ZnmVRR//hyQ9mHYFTRltIUmXSlooaWFfX18t68uFYmcbmxz8ZrlU6+D/KlAG5gCrgM+NtmBEzIuIuRExt1Qq1ai8/ChOcPCb5VVNgz8iVkfEYETsBL4OHF/L/dtLCh7OaZZbNQ1+SdOGvTwbeHi0Za26ip1t7BgM+gc8pNMsbyqalnlvSLoBOAnokbQC+DvgJElzSGb9XA78WbX2b2Mbmq9n07YBOou+/aJZnlQt+CPi/BGav1mt/dmeeXFq5v5BphYzLsbMasqf3M2poRuu+wKvWf44+HNq6IzfwW+WPw7+nPKc/Gb55eDPqaLP+M1yy8GfU+7qMcsvB39OFSe4q8csrxz8OVXo8Bm/WV45+HOqtUVMbG9l0zYHv1neOPhzrDihjc3bHfxmeePgz7FkambP1WOWNw7+HCt0trJp246syzCzGnPw51ixs803XDfLIQd/jvkuXGb55ODPsYKD3yyXHPw51l3oYO2mfiIi61LMrIYc/Dk2q1Rky/ZBVm/oz7oUM6shB3+OlXsKACzt25RxJWZWSw7+HCv3JrfecvCb5YuDP8d6J3VS7Gxj6RoHv1meOPhzTBLlUoGlfZuzLsXMasjBn3PlUtFdPWY54+DPuXJvkVXrt3k8v1mOOPhzrlxKRvY84e4es9xw8OdcueSRPWZ54+DPuRlTu2htEcsc/Ga54eDPuc62VmZ0d3lkj1mOtGVdgGWvXCrwwIp13LxoxYttB06ewImzezKsysyqxcFvvObg/fn5kjV85IcPvKx94SdPpafYmVFVZlYtVQt+SdcBZwBrIuLotK0b+D4wE1gOvDsiXqhWDVaZD711NmcfO52hSToXPvk8f/2DB3h8zSYHv1kTqmYf/7eA03ZpuwJYEBGHAwvS15axlhZxSHcXM6Ymj9fPmgp4pI9Zs6pa8EfEb4Dnd2k+E5ifPp8PnFWt/dvem7bfBCa2t7J0jS/4mjWjWo/qOSAiVgGkX3tHW1DSpZIWSlrY19dXswIt+QtgVqngM36zJlW3wzkjYl5EzI2IuaVSKetycsdz+Jg1r1oH/2pJ0wDSr2tqvH+rULlUZOW6rWzdPph1KWa2j9U6+G8DLk6fXwzcWuP9W4XKvQUi4Im17uc3azZVC35JNwB3A0dIWiHpEuAa4G2SHgPelr62OuQ5fMyaV9XG8UfE+aO8dUq19mn7zmE9BSQHv1kzqtuLu5atCe2tHDxloufwMWtCDn4bVblU9P14zZqQ5+qxUZVLRe5Z9hwPr1w/6jKdbS3M7i0iqYaVmdl4OPhtVEccOIltO3ZyxpfuGnO5eRe9jre/6sAaVWVm4+Xgt1GdNWc6pUmd7BjYOeL7OwP+4vpFPPzMBge/WQNx8NuoOtpaeOsRo86qAcAh3V0e+WPWYHxx18bFF4DNGo+D38alXCrwxNrN7NwZWZdiZhVy8Nu4lEtF+gd2snLd1qxLMbMKOfhtXMq9ntrBrNE4+G1cXprTx5/wNWsUDn4bl+5CB1O62n3Gb9ZAHPw2bh7ZY9ZYHPw2bsltGt3VY9YoHPw2buVSkbWb+lm/ZUfWpZhZBfzJXRu3oQu8j63ZyLEzptR8/yK5QbyZVcbBb+M2Ox3See7X7s5k/xPaW7j9L9/8Yh1mNjYHv43boVO7uOacV7NmY3/N972pf4B5v1nG4qdecPCbVcjBb+MmifOOn5HJvgcGd/Kt/1ju4aRme8AXd62htbW2MLOni2UeVWRWMQe/NbxyqegzfrM94OC3hlcuFXnquS3sGBz5hjFm9nIOfmt45d4CAzuDJ5/bknUpZg3BwW8N76WJ4tzdY1YJB781vFkOfrM94uC3hlfsbOOA/TpZusYje8wq4eC3puCRPWaVc/BbUxgK/gjf+9dsdxz81hTKpQIbtw3Qt6n200aYNZpMpmyQtBzYCAwCAxExN4s6rHkM3fv33K/eTWebz2esefzTOa/muJnd+3SbWc7V89aIWJvh/q2JzD20m/fMPYSN/b4ngDWXie2t+3ybnqTNmsLEjlY+fe5rsi7DrCFk9TdxAHdKWiTp0pEWkHSppIWSFvb19dW4PDOz5pVV8J8YEa8F3gF8UNJbdl0gIuZFxNyImFsqlWpfoZlZk8ok+CPimfTrGuAW4Pgs6jAzy6OaB7+kgqRJQ8+BtwMP17oOM7O8yuLi7gHALZKG9v+9iLgjgzrMzHKp5sEfEcuAY2q9XzMzS/iTLmZmOePgNzPLGTXCpFaS+oAn93L1HqDePyHsGvedRqjTNe4brnH3Do2IV4yHb4jgHw9JC+t9LiDXuO80Qp2ucd9wjXvPXT1mZjnj4Dczy5k8BP+8rAuogGvcdxqhTte4b7jGvdT0ffxmZvZyeTjjNzOzYRz8ZmY509TBL+k0SY9KelzSFVnXAyDpEEm/lLRE0iOSLkvbuyX9TNJj6dcpGdfZKun/Sbq9HutLa9pf0k2S/pAezzfUW52S/ir9d35Y0g2SJmRdo6TrJK2R9PCwtlFrkvTx9GfoUUl/nHGdn0n/vR+UdIuk/bOsc6Qah733N5JCUk+WNY6kaYNfUivwZZI5/48Czpd0VLZVATAAfCQijgROILkfwVHAFcCCiDgcWJC+ztJlwJJhr+utPoAvAndExB+RzP+0hDqqU9J04MPA3Ig4GmgFzquDGr8FnLZL24g1pf83zwNela7zlfRnK6s6fwYcHRGvAf4T+HjGdY5UI5IOAd4GPDWsLctj+TJNG/wkc/w/HhHLImI7cCNwZsY1ERGrImJx+nwjSVhNJ6ltfrrYfOCsTAoEJB0M/AnwjWHNdVMfgKT9gLcA3wSIiO0RsY46q5NkIsSJktqALuAZMq4xIn4DPL9L82g1nQncGBH9EfEE8Dg1un/GSHVGxJ0RMZC+vAc4OMs6RzmWAJ8HPkpyt8EhmR3LXTVz8E8Hnh72ekXaVjckzQSOBe4FDoiIVZD8cgB6MyztCyT/aXcOa6un+gBmAX3Av6RdUt9I7+9QN3VGxErgsyRnfauA9RFxZz3VOMxoNdXzz9H7gX9Nn9dNnZLeCayMiAd2eatuamzm4NcIbXUzdlVSEbgZuDwiNmRdzxBJZwBrImJR1rXsRhvwWuCrEXEssJn66H56UdpPfiZwGHAQUJB0YbZV7bG6/DmSdCVJt+n1Q00jLFbzOiV1AVcCfzvS2yO0ZXIsmzn4VwCHDHt9MMmf2ZmT1E4S+tdHxI/S5tWSpqXvTwPWZFTeicA7JS0n6R47WdJ366i+ISuAFRFxb/r6JpJfBPVU56nAExHRFxE7gB8Bb6yzGoeMVlPd/RxJuhg4A7ggXvogUr3UWSb5Rf9A+jN0MLBY0oHUT41NHfy/Aw6XdJikDpKLKrdlXBOSRNIvvSQirh321m3Axenzi4Fba10bQER8PCIOjoiZJMfsFxFxYb3UNyQingWelnRE2nQK8Hvqq86ngBMkdaX/7qeQXNOppxqHjFbTbcB5kjolHQYcDtyXQX1AMlIP+BjwzojYMuytuqgzIh6KiN6ImJn+DK0AXpv+f62LGocKbdoHcDrJlf+lwJVZ15PW9CaSP+8eBO5PH6cDU0lGUzyWfu2ug1pPAm5Pn9djfXOAhemx/DEwpd7qBD4F/IHkvtLfATqzrhG4geSaww6SYLpkrJpIui6WAo8C78i4zsdJ+smHfna+lmWdI9W4y/vLgZ6sj+WuD0/ZYGaWM83c1WNmZiNw8JuZ5YyD38wsZxz8ZmY54+A3M8sZB79ZFUg6aWhmU7N64+A3M8sZB7/lmqQLJd0n6X5J/5zeh2CTpM9JWixpgaRSuuwcSfcMmwt+Sto+W9LPJT2QrlNON1/US/cLuD799C6SrpH0+3Q7n83oW7ccc/Bbbkk6EngPcGJEzAEGgQuAArA4Il4L/Br4u3SVbwMfi2Qu+IeGtV8PfDkijiGZi2dV2n4scDnJ/SBmASdK6gbOBl6Vbucfqvk9mo3EwW95dgrwOuB3ku5PX88imY76++ky3wXeJGkysH9E/Dptnw+8RdIkYHpE3AIQEdvipTlk7ouIFRGxk2R6gZnABmAb8A1J5wDD55sxqwkHv+WZgPkRMSd9HBERV42w3Fjzmow01e6Q/mHPB4G2SG4icjzJ7KxnAXfsWclm4+fgtzxbAJwrqRdevO/soSQ/F+emy7wXuCsi1gMvSHpz2n4R8OtI7qWwQtJZ6TY60znZR5Teh2FyRPyUpBtozj7/rsx2oy3rAsyyEhG/l/RJ4E5JLSQzLH6Q5KYur5K0CFhPch0AkumKv5YG+zLgfWn7RcA/S7o63ca7xtjtJOBWSRNI/lr4q338bZntlmfnNNuFpE0RUcy6DrNqcVePmVnO+IzfzCxnfMZvZpYzDn4zs5xx8JuZ5YyD38wsZxz8ZmY58/8BKigW17Zz0QcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(loss_values)\n",
    "plt.title('loss vs. no. of epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss %')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e1f395",
   "metadata": {},
   "source": [
    "**2) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea231e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
